# OPLib æµå¼æ•°æ®æ¶æ„åˆ†æä¸æ”¹é€ å»ºè®®

## æ‰§è¡Œæ‘˜è¦

åŸºäºå¯¹å½“å‰ä»£ç åº“çš„å…¨é¢åˆ†æï¼Œæœ¬æ–‡æ¡£ä»ç³»ç»Ÿæ¶æ„å¸ˆè§’åº¦è¯„ä¼°äº†å°†OPLibä»ç¦»çº¿CSVæ•°æ®å¤„ç†è¿ç§»åˆ°åœ¨çº¿IoTæ•°æ®æµå¤„ç†æ‰€éœ€çš„æ¶æ„æ”¹é€ ã€‚æ ¸å¿ƒç»“è®ºæ˜¯ï¼š**ä¸éœ€è¦Lambdaæˆ–Kappaæ¶æ„çš„å®Œæ•´é‡æ„**ï¼Œå½“å‰åŸºäºåˆ†å±‚çš„æ¶æ„å·²å…·å¤‡è‰¯å¥½çš„æ‰©å±•æ€§åŸºç¡€ï¼Œä»…éœ€å¼ºåŒ–ç°æœ‰æ¶æ„çš„æµå¼èƒ½åŠ›ã€‚

---

## 1. å½“å‰æ¶æ„è¯„ä¼°

### 1.1 ç°æœ‰æ¶æ„ä¼˜åŠ¿

**äº”å±‚åˆ†å±‚æ¶æ„**
```
APIå±‚ â†’ å·¥ä½œæµç®¡ç†å±‚ â†’ æ ¸å¿ƒä¸šåŠ¡å±‚ â†’ å·¥å‚å±‚ â†’ é…ç½®ç®¡ç†å±‚
```

**æ ¸å¿ƒç‰¹ç‚¹ï¼š**
- âœ… **å·²å®ç°æ¥å£æŠ½è±¡**ï¼š`BaseDataSource` å·²æŠ½è±¡æ•°æ®æºæ¥å£
- âœ… **å·¥å‚æ¨¡å¼**ï¼šç»„ä»¶é€šè¿‡å·¥å‚åŠ¨æ€åˆ›å»ºï¼Œæ”¯æŒè¿è¡Œæ—¶æ›¿æ¢
- âœ… **é…ç½®é©±åŠ¨**ï¼šä¸šåŠ¡æµç¨‹å®Œå…¨é€šè¿‡YAMLé…ç½®ï¼Œæ— éœ€æ”¹ä»£ç 
- âœ… **ç±»å‹å®‰å…¨**ï¼šä½¿ç”¨TypedDictæä¾›å¼ºç±»å‹çº¦æŸ
- âœ… **æ•°æ®æµç®¡ç†**ï¼šå·²æœ‰ `DataFlowManager` å’Œ `DataFlowMonitor`

### 1.2 ç°æœ‰æµå¼æ”¯æŒåŸºç¡€è®¾æ–½

é€šè¿‡ä»£ç åˆ†æå‘ç°ï¼Œç³»ç»Ÿå·²ç»å…·å¤‡éƒ¨åˆ†æµå¼å¤„ç†çš„åŸºçŸ³ï¼š

```python
# 1. æ¥å£å±‚å·²æ”¯æŒæµå¼æ•°æ®æº
class BaseDataSource:
    @abstractmethod
    def read(self, **kwargs) -> DataSourceOutput: pass

# 2. Kafkaæ”¯æŒå·²é¢„ç•™ï¼ˆæœªå®ç°ï¼‰
class KafkaDataSource(BaseDataSource):
    def read(self, **kwargs) -> DataSourceOutput:
        raise WorkflowError("Kafkaæ•°æ®æºå°šæœªå®ç°")

# 3. å·¥ä½œæµè¾“å…¥æ¨¡å‹å·²æ”¯æŒåœ¨çº¿æ¨¡å¼æ ‡è¯†
class WorkflowInputs(BaseModel):
    file_path: Optional[str] = None
    online_data: Optional[bool] = None  # âš ï¸ å·²å®šä¹‰ä½†æœªä½¿ç”¨
```

### 1.3 å½“å‰æ•°æ®æµç¨‹ï¼ˆæ‰¹å¤„ç†æ¨¡å¼ï¼‰

```mermaid
graph LR
    A[CSVæ–‡ä»¶] --> B[readä¸€æ¬¡æ€§åŠ è½½]
    B --> C[DataFrameè½¬æ¢]
    C --> D[æ‰¹å¤„ç†åˆ†æ]
    D --> E[å®Œæ•´ç»“æœè¾“å‡º]
```

**ç‰¹å¾ï¼š**
- ä¸€æ¬¡æ€§è¯»å–å®Œæ•´æ•°æ®é›†
- å…¨éƒ¨æ•°æ®åŠ è½½åˆ°å†…å­˜
- æ‰€æœ‰å¤„ç†ç¯èŠ‚é’ˆå¯¹å®Œæ•´æ•°æ®é›†

---

## 2. Lambdaæ¶æ„ vs Kappaæ¶æ„å¯¹æ¯”

### 2.1 Lambdaæ¶æ„

**æ ¸å¿ƒæ€æƒ³ï¼š** æ‰¹å¤„ç†å±‚ï¼ˆå†å²å‡†ç¡®æ€§ï¼‰+ é€Ÿåº¦å±‚ï¼ˆå®æ—¶æ€§ï¼‰+ æœåŠ¡å±‚ï¼ˆç»Ÿä¸€æŸ¥è¯¢ï¼‰

```
æ‰¹å¤„ç†å±‚ï¼ˆBatch Layerï¼‰
    â†“
æœåŠ¡å±‚ï¼ˆServing Layerï¼‰ â† é€Ÿåº¦å±‚ï¼ˆSpeed Layerï¼‰
```

**é€‚ç”¨åœºæ™¯ï¼š**
- éœ€è¦ç¦»çº¿æ‰¹å¤„ç†ä¸åœ¨çº¿æµå¤„ç†åŒæ—¶æ»¡è¶³
- å†å²æ•°æ®å‡†ç¡®æ€§è¦æ±‚é«˜
- éœ€è¦ç»Ÿä¸€çš„å†å²+å®æ—¶æŸ¥è¯¢è§†å›¾

**å¤æ‚æ€§ï¼š**
- âš ï¸ ä¸¤å¥—ä»£ç è·¯å¾„
- âš ï¸ ä¸¤ä¸ªæ•°æ®ç®¡é“
- âš ï¸ éœ€è¦åˆå¹¶é€»è¾‘ï¼ˆ"Lambda = Batch âŠ” Speed"ï¼‰

### 2.2 Kappaæ¶æ„

**æ ¸å¿ƒæ€æƒ³ï¼š** å•ä¸€æµå¤„ç†ç®¡é“ï¼Œå†å²æ•°æ®é€šè¿‡å›æ”¾å®ç°

```
æµå¼æ•°æ®æº â†’ æµå¤„ç†å¼•æ“ â†’ ç»“æœå­˜å‚¨
                â†“
         (å†å²å›æ”¾æ¨¡å¼)
```

**é€‚ç”¨åœºæ™¯ï¼š**
- æµå¤„ç†èƒ½åŠ›è¶³å¤Ÿå¼º
- å†å²æŸ¥è¯¢éœ€æ±‚ä¸é¢‘ç¹
- æ¶æ„ç®€æ´æ€§ä¼˜å…ˆ

**å¤æ‚æ€§ï¼š**
- âœ… å•ä¸€ä»£ç è·¯å¾„
- âœ… ç»Ÿä¸€æ•°æ®æ¨¡å‹
- âš ï¸ å†å²æ•°æ®æŸ¥è¯¢æ€§èƒ½æœ‰é™

---

## 3. æ¶æ„å»ºè®®ï¼šæ”¹è¿›å‹å•ç®¡é“æ¶æ„ï¼ˆæ¨èï¼‰ğŸŒŸ

### 3.1 æ ¸å¿ƒç†å¿µ

**ä¸é‡‡ç”¨Lambda/Kappaï¼Œè€Œæ˜¯å¢å¼ºç°æœ‰äº”å±‚æ¶æ„çš„æµå¼èƒ½åŠ›**

**åŸå› åˆ†æï¼š**

1. **ä¸šåŠ¡åœºæ™¯ç‰¹ç‚¹**
   - çƒ­å‹ç½å›ºåŒ–å·¥è‰ºåˆ†æåœºæ™¯ä¸­ï¼Œæ•°æ®æ˜¯**æœ‰é™çš„æ—¶åºæ•°æ®é›†**
   - ä¸€ä¸ªå›ºåŒ–å‘¨æœŸï¼ˆå‡ å°æ—¶åˆ°åå‡ å°æ—¶ï¼‰çš„æ•°æ®é‡ç›¸å¯¹å¯æ§
   - ä¸æ˜¯æ— é™æµï¼Œè€Œæ˜¯**æœ‰å§‹æœ‰ç»ˆçš„å‘¨æœŸæ•°æ®æµ**

2. **ç°æœ‰æ¶æ„ä¼˜åŠ¿**
   - äº”å±‚æ¶æ„å¤©ç„¶æ”¯æŒæ•°æ®æºäº’æ¢
   - å·¥ä½œæµç¼–æ’å™¨å·²ç»å®ç°äº†ä»»åŠ¡é—´çš„ä¾èµ–ç®¡ç†
   - é…ç½®é©±åŠ¨çš„è®¾è®¡ä½¿å¾—æ·»åŠ æ–°æ•°æ®æºæ— éœ€æ”¹æ ¸å¿ƒä»£ç 

3. **æ”¹é€ æˆæœ¬è¯„ä¼°**
   - Lambdaæ¶æ„ï¼šéœ€è¦å¼•å…¥æ‰¹å¤„ç†+é€Ÿåº¦å±‚åŒç®¡é“ï¼Œå¤æ‚åº¦é«˜
   - Kappaæ¶æ„ï¼šéœ€è¦å¼•å…¥æµå¤„ç†å¼•æ“ï¼Œä½†ç°æœ‰ä¸šåŠ¡é€»è¾‘ä¸»è¦é¢å‘æ‰¹å¤„ç†
   - **å¢å¼ºç°æœ‰æ¶æ„**ï¼šåªéœ€å®ç°Kafkaæ•°æ®æºå’ŒåŠ è£…æ—¶é—´çª—å£ç¼“å†²

### 3.2 æ”¹è¿›æ–¹æ¡ˆè¯¦ç»†è®¾è®¡

#### æ–¹æ¡ˆAï¼šå¾®æ‰¹æ¬¡ç¼“å†²ç­–ç•¥ï¼ˆæ¨èï¼‰

**è®¾è®¡æ€è·¯ï¼š**
```
IoTæ•°æ®æº â†’ æ—¶é—´çª—å£ç¼“å†²å™¨ â†’ å¾®æ‰¹æ¬¡ â†’ ç°æœ‰å¤„ç†æµæ°´çº¿
```

**å®ç°ç­–ç•¥ï¼š**

```yaml
# config/workflow_config.yaml - å¢å¼ºé…ç½®ç¤ºä¾‹
workflows:
  curing_analysis_online:
    name: "åœ¨çº¿å›ºåŒ–åˆ†æå·¥ä½œæµ"
    parameters:
      online_data: true
      buffer_window_minutes: 5  # 5åˆ†é’Ÿå¾®æ‰¹æ¬¡
    
    workflow:
      - layer: "data_source"
        tasks:
          - id: "load_online_data"
            implementation: "kafka"
            algorithm: "micro_batch_reader"
            inputs:
              topic: "sensor_data"
              window_minutes: 5
              max_records: 1000
            depends_on: []
      
      # åç»­å¤„ç†å±‚ä¿æŒä¸å˜ï¼
      - layer: "data_processing"
        tasks:
          - id: "sensor_grouping"
            implementation: "data_grouper"
            algorithm: "sensor_grouper"
            depends_on: ["load_online_data"]
```

**ä¼˜åŠ¿ï¼š**
- âœ… æœ€å¤§åŒ–å¤ç”¨ç°æœ‰å¤„ç†é€»è¾‘
- âœ… æœ€å°åŒ–æ¶æ„æ”¹åŠ¨
- âœ… é€šè¿‡Kafkaæ¶ˆè´¹ç»„æ”¯æŒæ°´å¹³æ‰©å±•
- âœ… ä¿æŒç»“æœä¸€è‡´æ€§ï¼ˆåŸºäºæ—¶é—´çª—å£ï¼‰

#### æ–¹æ¡ˆBï¼šæ»‘åŠ¨çª—å£å¤„ç†ï¼ˆé«˜çº§ï¼‰

**é€‚ç”¨åœºæ™¯ï¼š** éœ€è¦å®æ—¶è¶‹åŠ¿åˆ†æ

```python
class SlidingWindowProcessor(BaseDataProcessor):
    """æ»‘åŠ¨çª—å£å¤„ç†å™¨"""
    
    def __init__(self, window_size: int = 100, slide_size: int = 10):
        self.window_size = window_size  # çª—å£å¤§å°
        self.slide_size = slide_size    # æ»‘åŠ¨æ­¥é•¿
        
    def process(self, data: DataSourceOutput) -> SensorGroupingOutput:
        # ä½¿ç”¨æ—¶é—´çª—å£å†…çš„æ•°æ®
        windowed_data = self._extract_window(data)
        return super().process(windowed_data)
```

#### æ–¹æ¡ˆCï¼šåŒæ¨¡å¼æ”¯æŒï¼ˆæœ€çµæ´»ï¼‰

**è®¾è®¡ï¼š** æ ¹æ® `online_data` æ ‡å¿—è‡ªåŠ¨é€‰æ‹©å¤„ç†æ¨¡å¼

```python
# src/data/sources/smart_source.py
class SmartDataSource(BaseDataSource):
    """æ™ºèƒ½æ•°æ®æº - æ ¹æ®æ ‡å¿—é€‰æ‹©æ‰¹å¤„ç†æˆ–æµå¤„ç†"""
    
    def __init__(self, **kwargs):
        self.online_mode = kwargs.get("online_data", False)
        if self.online_mode:
            self.source = KafkaDataSource(**kwargs)
        else:
            self.source = CSVDataSource(**kwargs)
    
    def read(self, **kwargs) -> DataSourceOutput:
        return self.source.read(**kwargs)
```

---

## 4. å®æ–½è·¯çº¿å›¾

### é˜¶æ®µ1ï¼šå®ç°æµå¼æ•°æ®æºé€‚é…å™¨ï¼ˆ1-2å‘¨ï¼‰

**ç›®æ ‡ï¼š** å®ç°Kafka/IoTæ•°æ®æºï¼Œå¯¹æ¥ç°æœ‰æµæ°´çº¿

**ä»»åŠ¡æ¸…å•ï¼š**
```python
# 1. å®Œå–„KafkaDataSourceå®ç°
class KafkaDataSource(BaseDataSource):
    def __init__(self, topic: str, window_minutes: int = 5, **kwargs):
        self.topic = topic
        self.window_minutes = window_minutes
        # Kafkaæ¶ˆè´¹è€…åˆå§‹åŒ–
        self.consumer = KafkaConsumer(topic, **kwargs)
    
    def read(self, **kwargs) -> DataSourceOutput:
        # è¯»å–æ—¶é—´çª—å£å†…çš„æ•°æ®
        records = []
        end_time = time.time() + (self.window_minutes * 60)
        
        for msg in self.consumer:
            if time.time() > end_time:
                break
            records.append(msg)
        
        # è½¬æ¢ä¸ºä¸CSVç›¸åŒçš„DataSourceOutputæ ¼å¼
        return self._format_as_data_source_output(records)
```

**éªŒè¯æ ‡å‡†ï¼š**
- Kafkaæ•°æ®æºèƒ½è¾“å‡ºæ ‡å‡†`DataSourceOutput`æ ¼å¼
- å·¥ä½œæµé…ç½®å¯æŒ‡å®š`implementation: "kafka"`
- ç°æœ‰å¤„ç†å±‚æ— éœ€æ”¹åŠ¨å³å¯å¤„ç†

### é˜¶æ®µ2ï¼šæ·»åŠ æµå¼å¤„ç†èƒ½åŠ›ï¼ˆ2-3å‘¨ï¼‰

**ç›®æ ‡ï¼š** ä¸ºåˆ†æå±‚å¢åŠ å®æ—¶å¤„ç†æ”¯æŒ

**å…³é”®æ¨¡å—ï¼š**

```python
# src/analysis/streaming/
class StreamingAnalysisBase:
    """æµå¼åˆ†æåŸºç±»"""
    
    def analyze_incremental(self, 
                           new_data: DataFrame,
                           previous_state: Dict) -> AnalysisResult:
        """å¢é‡åˆ†æ - ä½¿ç”¨æ»‘åŠ¨çª—å£çŠ¶æ€"""
        pass

# å…·ä½“å®ç°
class StreamingRuleEngineAnalyzer(RuleEngineAnalyzer, StreamingAnalysisBase):
    """æµå¼è§„åˆ™å¼•æ“ - æ”¯æŒå¢é‡è§„åˆ™æ£€æŸ¥"""
    pass
```

### é˜¶æ®µ3ï¼šä¼˜åŒ–ä¸ç›‘æ§ï¼ˆ1-2å‘¨ï¼‰

**ä»»åŠ¡ï¼š**
- æ€§èƒ½ä¼˜åŒ–ï¼ˆçª—å£èšåˆä¼˜åŒ–ï¼‰
- æµå¼ç›‘æ§ï¼ˆå»¶è¿Ÿã€ååé‡ï¼‰
- å®¹é”™æœºåˆ¶ï¼ˆæ•°æ®è¡¥å‘ã€ä¸€è‡´æ€§ï¼‰

---

## 5. æŠ€æœ¯é€‰å‹å»ºè®®

### 5.1 æµå¤„ç†å¼•æ“é€‰æ‹©

| æ–¹æ¡ˆ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| **Kafka Streaming** | ä¸ç°æœ‰Kafkaé›†æˆå¥½ï¼Œä½å»¶è¿Ÿ | åŠŸèƒ½ç›¸å¯¹ç®€å• | â­â­â­â­â­ |
| Apache Flink | åŠŸèƒ½å¼ºå¤§ï¼ŒçŠ¶æ€ç®¡ç†å®Œå–„ | å¤æ‚åº¦é«˜ï¼Œèµ„æºæ¶ˆè€—å¤§ | â­â­â­ |
| Apache Spark Streaming | æˆç†Ÿç¨³å®šï¼Œç”Ÿæ€ä¸°å¯Œ | å»¶è¿Ÿç›¸å¯¹è¾ƒé«˜ | â­â­â­â­ |
| **è‡ªå®šä¹‰å¾®æ‰¹æ¬¡** | ç®€å•å¯æ§ï¼Œæ˜“ç»´æŠ¤ | åŠŸèƒ½æœ‰é™ | â­â­â­â­â­ |

**æ¨èï¼šKafka Streams + å¾®æ‰¹æ¬¡ç¼“å†²**  
- ä¸ç°æœ‰KafkaåŸºç¡€è®¾æ–½ä¸€è‡´
- å»¶è¿Ÿå¯æ§åˆ¶åœ¨ç§’çº§
- å¤æ‚åº¦é€‚ä¸­

### 5.2 çŠ¶æ€ç®¡ç†

**é€‰é¡¹1ï¼šå†…å­˜çŠ¶æ€ï¼ˆç®€å•ï¼‰**
```python
class InMemoryStateStore:
    """å†…å­˜çŠ¶æ€å­˜å‚¨ - é€‚åˆçŸ­æœŸçŠ¶æ€"""
    def __init__(self, ttl_seconds: int = 3600):
        self.store = {}
        self.ttl = ttl_seconds
```

**é€‰é¡¹2ï¼šRedisçŠ¶æ€ï¼ˆæ¨èï¼‰**
```python
class RedisStateStore:
    """RedisçŠ¶æ€å­˜å‚¨ - æ”¯æŒåˆ†å¸ƒå¼å’ŒæŒä¹…åŒ–"""
    def __init__(self, redis_client):
        self.client = redis_client
```

**é€‰é¡¹3ï¼šKafka StreamsçŠ¶æ€å­˜å‚¨**
```python
# ä½¿ç”¨Kafka Streamsçš„å†…ç½®çŠ¶æ€å­˜å‚¨
from kafka.streams import StreamsBuilder
builder = StreamsBuilder()
# è‡ªåŠ¨ç®¡ç†çŠ¶æ€
```

---

## 6. æ¶æ„æ¼”è¿›è·¯å¾„

### 6.1 çŸ­æœŸï¼ˆ3ä¸ªæœˆå†…ï¼‰

```
å½“å‰ï¼šæ‰¹å¤„ç†CSV
  â†“
å¢å¼ºï¼šæ‰¹å¤„ç†CSV + æµå¤„ç†Kafkaï¼ˆåŒæ¨¡å¼ï¼‰
  â†“
ç»Ÿä¸€ï¼šæµå¤„ç†ä¸ºä¸»ï¼ˆKafkaä¸ºä¸»è¦æ•°æ®æºï¼‰
```

### 6.2 ä¸­æœŸï¼ˆ6-12ä¸ªæœˆï¼‰

**å¦‚æœéœ€è¦å†å²åˆ†æéœ€æ±‚å¢å¤šï¼š**

```
æµå¤„ç† + å†å²å­˜å‚¨
  â†“
Lambdaæ¶æ„ï¼ˆå¼•å…¥HDFS/å¯¹è±¡å­˜å‚¨ä½œä¸ºæ‰¹å¤„ç†å±‚ï¼‰
```

### 6.3 é•¿æœŸï¼ˆ12ä¸ªæœˆä»¥ä¸Šï¼‰

**å¦‚æœæ•°æ®é‡è¾¾åˆ°å¤§æ•°æ®è§„æ¨¡ï¼š**

```
è€ƒè™‘å¼•å…¥Flink/Sparkç­‰æµå¤„ç†å¼•æ“
  â†“
å®Œæ•´çš„Kappaæˆ–Lambdaæ¶æ„
```

---

## 7. å…³é”®è®¾è®¡å†³ç­–

### 7.1 æ•°æ®ä¸€è‡´æ€§ä¿è¯

**é—®é¢˜ï¼š** æµå¤„ç†å¦‚ä½•ä¿è¯ç»“æœä¸æ‰¹å¤„ç†ä¸€è‡´ï¼Ÿ

**æ–¹æ¡ˆï¼š**
1. **ä½¿ç”¨ç›¸åŒå¤„ç†ç®—æ³•**ï¼šæµå¼æ•°æ®é€šè¿‡æ—¶é—´çª—å£èšåˆåï¼Œè°ƒç”¨å®Œå…¨ç›¸åŒçš„æ‰¹å¤„ç†é€»è¾‘
2. **çŠ¶æ€ç´¯ç§¯**ï¼šä¿æŒä¸€ä¸ªç´¯ç§¯çŠ¶æ€ï¼Œç¡®ä¿æ¯ä¸ªçª—å£ç»“æŸæ—¶çš„ç»“æœä¸æ‰¹å¤„ç†ç­‰ä»·

```python
# ä¼ªä»£ç ç¤ºä¾‹
def process_window(window_data):
    # ä½¿ç”¨ä¸æ‰¹å¤„ç†ç›¸åŒçš„å¤„ç†é€»è¾‘
    result = sensor_grouping.process(window_data)
    result = stage_detection.process(result)
    result = rule_engine.analyze(result)
    return result
```

### 7.2 å®¹é”™æœºåˆ¶

**å…³é”®ç‚¹ï¼š**
- æ¶ˆæ¯ç¡®è®¤ï¼šç¡®ä¿Kafkaæ¶ˆæ¯æ¶ˆè´¹çš„at-least-onceè¯­ä¹‰
- çŠ¶æ€æ¢å¤ï¼šå®šæœŸä¿å­˜å¤„ç†çŠ¶æ€ï¼Œæ”¯æŒé‡å¯åæ¢å¤
- æ•°æ®è¡¥å‘ï¼šæ”¯æŒä»æŸä¸ªæ—¶é—´ç‚¹é‡æ–°æ¶ˆè´¹

### 7.3 æ€§èƒ½ä¼˜åŒ–

**ç“¶é¢ˆåˆ†æï¼š**
1. **æ•°æ®æº**ï¼šKafkaæ¶ˆè´¹é€Ÿåº¦ï¼ˆå¯é€šè¿‡å¢åŠ åˆ†åŒºè§£å†³ï¼‰
2. **å¤„ç†é˜¶æ®µ**ï¼šè§„åˆ™å¼•æ“è®¡ç®—å¤æ‚åº¦ï¼ˆå¯å¹¶è¡ŒåŒ–ï¼‰
3. **ç»“æœè¾“å‡º**ï¼šæ–‡ä»¶å†™å…¥æˆ–æ•°æ®åº“å†™å…¥ï¼ˆå¼‚æ­¥åŒ–ï¼‰

**ä¼˜åŒ–ç­–ç•¥ï¼š**
```python
# 1. å¹¶è¡Œæ¶ˆè´¹
consumer_config = {
    'group_id': 'oplib_workers',
    'partition_assignment_strategy': 'round_robin'
}

# 2. æ‰¹é‡å†™å…¥
async def batch_write_results(results: List[Dict]):
    await asyncio.gather(*[write_result(r) for r in results])
```

---

## 8. ä¸Lambda/Kappaæ¶æ„çš„å¯¹æ¯”

### 8.1 ä¸ºä»€ä¹ˆä¸é€‰æ‹©Lambdaæ¶æ„

âŒ **ä¸æ¨èåŸå› ï¼š**
1. **ä¸šåŠ¡å¤æ‚åº¦**ï¼šéœ€è¦ç»´æŠ¤æ‰¹å¤„ç†å±‚å’Œé€Ÿåº¦å±‚ä¸¤å¥—ä»£ç 
2. **æ•°æ®åŒæ­¥**ï¼šéœ€è¦å¤„ç†å†å²æ•°æ®ä¸å®æ—¶æ•°æ®çš„åˆå¹¶é€»è¾‘
3. **èµ„æºæ¶ˆè€—**ï¼šä¸¤å¥—è®¡ç®—èµ„æºï¼Œæˆæœ¬é«˜
4. **å¯¹äºOPLibåœºæ™¯**ï¼šä¸€ä¸ªå›ºåŒ–å‘¨æœŸçš„æ•°æ®é‡ä¸å¤§ï¼Œä¸éœ€è¦å¼•å…¥Hadoopç”Ÿæ€

### 8.2 ä¸ºä»€ä¹ˆä¸å®Œå…¨é‡‡ç”¨Kappaæ¶æ„

âŒ **ä¸æ¨èåŸå› ï¼š**
1. **ç°æœ‰ä»£ç æŠ•èµ„**ï¼šå·²æœ‰å¤§é‡é’ˆå¯¹æ‰¹å¤„ç†çš„ä¼˜åŒ–ä»£ç 
2. **å†å²æŸ¥è¯¢éœ€æ±‚**ï¼šå›ºåŒ–å·¥è‰ºåˆ†æç»å¸¸éœ€è¦æŸ¥çœ‹å†å²å®Œæ•´ç»“æœ
3. **å®ç°å¤æ‚åº¦**ï¼šKappaéœ€è¦å®Œæ•´é‡å†™æ•°æ®å¤„ç†é€»è¾‘

### 8.3 ä¸ºä»€ä¹ˆæ¨èå¢å¼ºå‹æ¶æ„

âœ… **æ¨èåŸå› ï¼š**
1. **æœ€å¤§åŒ–æŠ•èµ„å›æŠ¥**ï¼šå¤ç”¨80%+ç°æœ‰ä»£ç 
2. **æ¸è¿›å¼æ¼”è¿›**ï¼šå¯ä»¥é€æ­¥è¿ç§»ï¼Œé™ä½é£é™©
3. **é…ç½®çµæ´»**ï¼šé€šè¿‡YAMLé…ç½®åˆ‡æ¢æ‰¹å¤„ç†/æµå¤„ç†æ¨¡å¼
4. **è¿ç»´ç®€å•**ï¼šå•ä¸€ç®¡é“ï¼Œæ— éœ€å¤æ‚çš„åè°ƒæœºåˆ¶

---

## 9. å®æ–½é£é™©è¯„ä¼°

### 9.1 æŠ€æœ¯é£é™©

| é£é™©é¡¹ | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|--------|------|------|----------|
| Kafkaæ€§èƒ½ç“¶é¢ˆ | é«˜ | ä¸­ | å¢åŠ åˆ†åŒºï¼Œç›‘æ§æ¶ˆè´¹å»¶è¿Ÿ |
| çŠ¶æ€ç®¡ç†å¤æ‚ | ä¸­ | ä¸­ | ä½¿ç”¨Redisç­‰æˆç†Ÿæ–¹æ¡ˆ |
| æ•°æ®ä¸€è‡´æ€§é—®é¢˜ | é«˜ | ä½ | å¼•å…¥å¹‚ç­‰æ€§æ£€æŸ¥å’ŒçŠ¶æ€éªŒè¯ |
| å†…å­˜æº¢å‡º | ä¸­ | ä½ | æµå¼å¤„ç†+å†…å­˜é™åˆ¶ |

### 9.2 ä¸šåŠ¡é£é™©

| é£é™©é¡¹ | å½±å“ | æ¦‚ç‡ | ç¼“è§£æªæ–½ |
|--------|------|------|----------|
| å®æ—¶æ€§ä¸è¾¾æ ‡ | ä¸­ | ä½ | æ€§èƒ½æµ‹è¯•å’Œä¼˜åŒ– |
| å†å²æ•°æ®æŸ¥è¯¢å›°éš¾ | ä¸­ | ä½ | ä¿ç•™å†å²ç»“æœå­˜å‚¨ |
| ç”¨æˆ·ä½“éªŒä¸‹é™ | ä½ | ä½ | ä¿æŒåŒæ¨¡å¼æ”¯æŒ |

---

## 10. æ€»ç»“ä¸å»ºè®®

### 10.1 æ ¸å¿ƒç»“è®º

**æ— éœ€Lambdaæˆ–Kappaæ¶æ„çš„å®Œæ•´é‡æ„**ã€‚å½“å‰äº”å±‚æ¶æ„å·²ç»å…·å¤‡è‰¯å¥½çš„å¯æ‰©å±•æ€§ï¼Œåªéœ€è¦ï¼š

1. âœ… **å®ç°Kafkaæ•°æ®æºé€‚é…å™¨** - å°†æµå¼æ•°æ®é€‚é…ä¸ºç°æœ‰æ¥å£
2. âœ… **æ·»åŠ æ—¶é—´çª—å£ç¼“å†²** - å°†æµå¼æ•°æ®è½¬æ¢ä¸ºå¾®æ‰¹æ¬¡
3. âœ… **ä¿æŒç°æœ‰å¤„ç†é€»è¾‘** - æœ€å¤§åŒ–ä»£ç å¤ç”¨

### 10.2 å®æ–½ä¼˜å…ˆçº§

**P0ï¼ˆå¿…é¡»ï¼‰ï¼š**
- å®ç°Kafkaæ•°æ®æº
- æ·»åŠ æµå¼é…ç½®æ”¯æŒ

**P1ï¼ˆé‡è¦ï¼‰ï¼š**
- æ€§èƒ½ä¼˜åŒ–
- ç›‘æ§ä»ªè¡¨æ¿

**P2ï¼ˆå¯é€‰ï¼‰ï¼š**
- æµå¼çŠ¶æ€ç®¡ç†ä¼˜åŒ–
- é«˜çº§æµå¼åˆ†æç®—æ³•

### 10.3 å…³é”®æˆåŠŸå› ç´ 

1. **ä¿æŒæ¥å£ä¸å˜** - ç¡®ä¿æ•°æ®æºå±‚çš„`DataSourceOutput`æ¥å£ç»Ÿä¸€
2. **é…ç½®é©±åŠ¨** - é€šè¿‡YAMLé…ç½®å®ç°æ‰¹å¤„ç†/æµå¤„ç†åˆ‡æ¢
3. **æ¸è¿›å¼è¿ç§»** - å…ˆæ”¯æŒåŒæ¨¡å¼ï¼Œå†é€æ­¥ä¼˜åŒ–

---

## é™„å½•Aï¼šå‚è€ƒå®ç°ç¤ºä¾‹

### A.1 Kafkaæ•°æ®æºå®Œæ•´å®ç°

```python
"""Kafkaæ•°æ®æºå®ç° - æµå¼æ•°æ®é€‚é…å™¨"""

from kafka import KafkaConsumer
from typing import Any, Dict, List
import pandas as pd
from datetime import datetime

from ...core.interfaces import BaseDataSource
from ...core.types import DataSourceOutput, Metadata

class KafkaDataSource(BaseDataSource):
    """Kafkaæ•°æ®æº - å®ç°æµå¼æ•°æ®åˆ°æ‰¹å¤„ç†çš„é€‚é…"""
    
    def __init__(self, topic: str, brokers: List[str], 
                 window_minutes: int = 5, **kwargs):
        super().__init__(**kwargs)
        self.topic = topic
        self.brokers = brokers
        self.window_minutes = window_minutes
        
        # åˆå§‹åŒ–Kafkaæ¶ˆè´¹è€…
        self.consumer = KafkaConsumer(
            self.topic,
            bootstrap_servers=self.brokers,
            **kwargs
        )
        
        self.algorithm = "kafka_windowed_reader"
    
    def read(self, **kwargs) -> DataSourceOutput:
        """è¯»å–æ—¶é—´çª—å£å†…çš„æµå¼æ•°æ®"""
        
        # 1. æ¶ˆè´¹æ—¶é—´çª—å£å†…çš„æ¶ˆæ¯
        messages = []
        end_time = datetime.now().timestamp() + (self.window_minutes * 60)
        
        for msg in self.consumer:
            if datetime.now().timestamp() > end_time:
                break
                
            # è§£ææ¶ˆæ¯
            message_data = self._parse_message(msg)
            messages.append(message_data)
        
        # 2. è½¬æ¢ä¸ºDataFrame
        df = pd.DataFrame(messages)
        
        # 3. è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
        data = df.to_dict('list')
        
        # 4. ç”Ÿæˆå…ƒæ•°æ®
        metadata: Metadata = {
            "source_type": "kafka_stream",
            "format": "sensor_data",
            "timestamp_column": "timestamp",
            "row_count": len(df),
            "column_count": len(df.columns),
            "columns": list(df.columns),
            "file_path": None,
            "created_at": None,
            "updated_at": None
        }
        
        return DataSourceOutput(
            data=data,
            metadata=metadata
        )
    
    def _parse_message(self, msg) -> Dict[str, Any]:
        """è§£æKafkaæ¶ˆæ¯"""
        # æ ¹æ®å®é™…æ¶ˆæ¯æ ¼å¼å®ç°
        import json
        return json.loads(msg.value)
    
    def validate(self) -> bool:
        """éªŒè¯Kafkaæ•°æ®æº"""
        try:
            # æµ‹è¯•è¿æ¥
            topics = self.consumer.topics()
            return self.topic in topics
        except:
            return False
```

### A.2 æµå¼åˆ†æå™¨åŸºç±»

```python
"""æµå¼åˆ†æå™¨åŸºç±» - æ”¯æŒå¢é‡å¤„ç†"""

from typing import Dict, Any, Optional
from abc import ABC, abstractmethod

class StreamingAnalysisBase(ABC):
    """æµå¼åˆ†æåŸºç±» - ç®¡ç†çŠ¶æ€å’Œå¢é‡æ›´æ–°"""
    
    def __init__(self, state_store: Optional[Any] = None):
        self.state_store = state_store  # çŠ¶æ€å­˜å‚¨
        self.window_state: Dict[str, Any] = {}  # çª—å£çŠ¶æ€
    
    @abstractmethod
    def analyze_incremental(self, 
                           new_data: Any,
                           previous_state: Dict[str, Any]) -> Dict[str, Any]:
        """å¢é‡åˆ†æ"""
        pass
    
    def update_state(self, key: str, value: Any):
        """æ›´æ–°çŠ¶æ€"""
        self.window_state[key] = value
        if self.state_store:
            self.state_store.save(key, value)
    
    def get_state(self, key: str) -> Any:
        """è·å–çŠ¶æ€"""
        if self.state_store:
            return self.state_store.load(key)
        return self.window_state.get(key)
```

---

**æ–‡æ¡£ç‰ˆæœ¬ï¼š** v1.0  
**æœ€åæ›´æ–°ï¼š** 2025-01-24  
**ä½œè€…ï¼š** AIç³»ç»Ÿæ¶æ„å¸ˆ
